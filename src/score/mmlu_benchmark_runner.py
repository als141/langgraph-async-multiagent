"""
MMLUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import os
import time
import json
import asyncio
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict

from .data_loader import MMLUDataLoader, MMLUProblem
from .mmlu_orchestrator import MMLUOrchestrator, DebateResult
from .mmlu_evaluator import MMLUEvaluator, BenchmarkReport


@dataclass
class BenchmarkConfig:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š"""
    dataset_path: str
    output_dir: str
    max_turns_per_question: int = 15
    questions_per_category: int = 25
    batch_size: int = 10
    max_concurrent: int = 1  # åŒæ™‚å®Ÿè¡Œæ•°ï¼ˆAPIåˆ¶é™è€ƒæ…®ï¼‰
    timeout_per_question: int = 300  # ç§’
    save_intermediate_results: bool = True
    log_dir: str = "logs"


@dataclass
class BenchmarkProgress:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é€²æ—ç®¡ç†"""
    completed_questions: List[str]
    failed_questions: List[str]
    current_batch: int
    start_time: Optional[float]
    
    def save_checkpoint(self, filepath: str):
        """é€²æ—ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        checkpoint_data = asdict(self)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    
    def load_checkpoint(self, filepath: str):
        """ä¿å­˜ã•ã‚ŒãŸé€²æ—ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.completed_questions = data.get('completed_questions', [])
                self.failed_questions = data.get('failed_questions', [])
                self.current_batch = data.get('current_batch', 0)
                self.start_time = data.get('start_time')
            return True
        return False


class MMLUBenchmarkRunner:
    """MMLUãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œå™¨"""
    
    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.data_loader = MMLUDataLoader(config.dataset_path)
        self.orchestrator = MMLUOrchestrator(
            max_turns=config.max_turns_per_question,
            log_dir=config.log_dir
        )
        self.evaluator = MMLUEvaluator()
        self.progress = BenchmarkProgress(
            completed_questions=[],
            failed_questions=[],
            current_batch=0,
            start_time=None
        )
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        self.output_path = Path(config.output_dir)
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        self.checkpoint_path = self.output_path / "checkpoint.json"
    
    async def run_full_benchmark(self, total_questions: Optional[int] = None) -> BenchmarkReport:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å®Œå…¨å®Ÿè¡Œ"""
        
        print("MMLU PROãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’é–‹å§‹ã—ã¾ã™...")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if total_questions:
            # æŒ‡å®šã•ã‚ŒãŸç·å•é¡Œæ•°ã§å®Ÿè¡Œ
            problems = self._load_problems_by_total(total_questions)
            print(f"ç·å•é¡Œæ•°æŒ‡å®šãƒ¢ãƒ¼ãƒ‰: {total_questions}å•")
        else:
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            problems = self.data_loader.stratified_sample(self.config.questions_per_category)
            print(f"ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: å„ã‚«ãƒ†ã‚´ãƒª{self.config.questions_per_category}å•")
            
        if not problems:
            raise ValueError("å•é¡Œãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        print(f"å¯¾è±¡å•é¡Œæ•°: {len(problems)}")
        
        # å®Ÿè¡Œé–‹å§‹
        self.progress.start_time = time.time()
        
        try:
            # ãƒãƒƒãƒå‡¦ç†ã§å®Ÿè¡Œ
            debate_results = await self._run_batch_processing(problems)
            
            # è©•ä¾¡å®Ÿè¡Œ
            execution_time = time.time() - self.progress.start_time
            report = self.evaluator.evaluate_batch_results(
                debate_results, 
                problems, 
                execution_time
            )
            
            # çµæœä¿å­˜
            await self._save_results(report, debate_results, problems)
            
            print(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†! å…¨ä½“æ­£ç­”ç‡: {report.overall_accuracy:.2%}")
            return report
            
        except Exception as e:
            print(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            await self._save_checkpoint()
            raise
    
    def _load_problems_by_total(self, total_questions: int) -> List[MMLUProblem]:
        """æŒ‡å®šã•ã‚ŒãŸç·å•é¡Œæ•°ã ã‘é †ç•ªã«èª­ã¿è¾¼ã¿"""
        all_problems = self.data_loader.load_and_validate()
        
        if total_questions > len(all_problems):
            print(f"è­¦å‘Š: æŒ‡å®šã•ã‚ŒãŸå•é¡Œæ•°{total_questions}ã¯åˆ©ç”¨å¯èƒ½ãªå•é¡Œæ•°{len(all_problems)}ã‚’è¶…ãˆã¦ã„ã¾ã™")
            return all_problems
        
        # æœ€åˆã®Nå•ã‚’è¿”ã™
        selected_problems = all_problems[:total_questions]
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆã‚’è¡¨ç¤º
        category_counts = {}
        for problem in selected_problems:
            category_counts[problem.category] = category_counts.get(problem.category, 0) + 1
        
        print("é¸æŠã•ã‚ŒãŸå•é¡Œã®ã‚«ãƒ†ã‚´ãƒªåˆ¥å†…è¨³:")
        for category, count in category_counts.items():
            print(f"  {category}: {count}å•")
        
        return selected_problems
    
    async def resume_benchmark(self) -> BenchmarkReport:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹"""
        
        if not self.progress.load_checkpoint(self.checkpoint_path):
            print("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦å®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            return await self.run_full_benchmark()
        
        print(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã—ã¾ã™...")
        print(f"å®Œäº†æ¸ˆã¿: {len(self.progress.completed_questions)}å•")
        print(f"å¤±æ•—: {len(self.progress.failed_questions)}å•")
        
        # æ®‹ã‚Šã®å•é¡Œã‚’å–å¾—
        problems = self.data_loader.stratified_sample(self.config.questions_per_category)
        remaining_problems = [
            p for p in problems 
            if p.question_id not in self.progress.completed_questions
        ]
        
        if not remaining_problems:
            print("ã™ã¹ã¦ã®å•é¡ŒãŒå®Œäº†æ¸ˆã¿ã§ã™ã€‚")
            # æ—¢å­˜ã®çµæœã‚’èª­ã¿è¾¼ã‚“ã§è¿”ã™
            return await self._load_existing_results()
        
        print(f"æ®‹ã‚Šå•é¡Œæ•°: {len(remaining_problems)}")
        
        # æ®‹ã‚Šã®å•é¡Œã‚’å®Ÿè¡Œ
        remaining_results = await self._run_batch_processing(remaining_problems)
        
        # æ—¢å­˜çµæœã¨çµåˆ
        all_results = await self._combine_results(remaining_results, problems)
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        execution_time = time.time() - self.progress.start_time
        report = self.evaluator.evaluate_batch_results(all_results, problems, execution_time)
        
        await self._save_results(report, all_results, problems)
        
        print(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†! å…¨ä½“æ­£ç­”ç‡: {report.overall_accuracy:.2%}")
        return report
    
    async def _run_batch_processing(self, problems: List[MMLUProblem]) -> List[DebateResult]:
        """ãƒãƒƒãƒå‡¦ç†ã§å•é¡Œã‚’å®Ÿè¡Œ"""
        
        all_results = []
        
        # ãƒãƒƒãƒã«åˆ†å‰²
        for i in range(0, len(problems), self.config.batch_size):
            batch = problems[i:i + self.config.batch_size]
            batch_num = i // self.config.batch_size + 1
            
            print(f"\nãƒãƒƒãƒ {batch_num}/{(len(problems) - 1) // self.config.batch_size + 1} ã‚’å®Ÿè¡Œä¸­...")
            
            # ãƒãƒƒãƒå†…ã®å•é¡Œã‚’ä¸¦åˆ—å®Ÿè¡Œï¼ˆåˆ¶é™ä»˜ãï¼‰
            semaphore = asyncio.Semaphore(self.config.max_concurrent)
            batch_tasks = []
            active_problems = []
            
            for problem in batch:
                if problem.question_id in self.progress.completed_questions:
                    continue  # ã‚¹ã‚­ãƒƒãƒ—æ¸ˆã¿å•é¡Œ
                    
                task = self._run_single_problem_with_semaphore(semaphore, problem)
                batch_tasks.append(task)
                active_problems.append(problem)
            
            # ãƒãƒƒãƒå†…ã®å•é¡Œã‚’ä¸€è¦§è¡¨ç¤ºï¼ˆä¸¦è¡Œå®Ÿè¡Œã•ã‚Œã‚‹å•é¡Œï¼‰
            if active_problems:
                print(f"ğŸ”„ {len(active_problems)}å•ã‚’ä¸¦è¡Œå®Ÿè¡Œé–‹å§‹:")
                for problem in active_problems:
                    short_question = problem.question_ja[:60] + "..." if len(problem.question_ja) > 60 else problem.question_ja
                    print(f"  - å•é¡Œ {problem.question_id}: {short_question}")
            
            # ãƒãƒƒãƒå®Ÿè¡Œ
            if batch_tasks:
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # çµæœå‡¦ç†
                for j, result in enumerate(batch_results):
                    problem = batch[j] if j < len(batch) else None
                    
                    if isinstance(result, Exception):
                        print(f"å•é¡Œ {problem.question_id if problem else 'unknown'} ã§ã‚¨ãƒ©ãƒ¼: {result}")
                        if problem:
                            self.progress.failed_questions.append(problem.question_id)
                        continue
                    
                    if result and problem:
                        all_results.append(result)
                        self.progress.completed_questions.append(problem.question_id)
                        
                        # é€²æ—è¡¨ç¤º
                        progress_pct = len(self.progress.completed_questions) / len(problems) * 100
                        print(f"  å•é¡Œ {problem.question_id} å®Œäº† ({progress_pct:.1f}%)")
            
            # ä¸­é–“ä¿å­˜
            if self.config.save_intermediate_results:
                await self._save_checkpoint()
                await self._save_intermediate_results(all_results, batch_num)
        
        return all_results
    
    async def _run_single_problem_with_semaphore(
        self, 
        semaphore: asyncio.Semaphore, 
        problem: MMLUProblem
    ) -> DebateResult:
        """ã‚»ãƒãƒ•ã‚©ä»˜ãã§å˜ä¸€å•é¡Œã‚’å®Ÿè¡Œ"""
        
        async with semaphore:
            try:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å®Ÿè¡Œ
                result = await asyncio.wait_for(
                    self.orchestrator.run_single_problem_debate(problem),
                    timeout=self.config.timeout_per_question
                )
                return result
                
            except asyncio.TimeoutError:
                print(f"å•é¡Œ {problem.question_id} ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
                # ãƒ€ãƒŸãƒ¼çµæœã‚’è¿”ã™
                return DebateResult(
                    question_id=problem.question_id,
                    final_conclusion="ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã«ã‚ˆã‚Šè­°è«–ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ",
                    full_transcript=[],
                    turn_count=0,
                    debate_duration=self.config.timeout_per_question,
                    facilitator_interventions=0,
                    consensus_score=0.0
                )
            
            except Exception as e:
                print(f"å•é¡Œ {problem.question_id} ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                raise
    
    async def _save_results(
        self, 
        report: BenchmarkReport, 
        debate_results: List[DebateResult], 
        problems: List[MMLUProblem]
    ):
        """çµæœã®ä¿å­˜"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_text = self.evaluator.generate_detailed_report(report)
        report_file = self.output_path / f"benchmark_report_{timestamp}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        # JSONå½¢å¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        json_file = self.output_path / f"benchmark_report_{timestamp}.json"
        self.evaluator.save_report_json(report, json_file)
        
        # è©³ç´°çµæœCSVä¿å­˜
        csv_file = self.output_path / f"detailed_results_{timestamp}.csv"
        self.evaluator.save_detailed_results_csv(report.detailed_results, csv_file)
        
        # è­°è«–çµæœã®ç”Ÿãƒ‡ãƒ¼ã‚¿ä¿å­˜
        raw_results = []
        for debate_result, problem in zip(debate_results, problems):
            raw_results.append({
                "problem": asdict(problem),
                "debate_result": asdict(debate_result)
            })
        
        raw_file = self.output_path / f"raw_results_{timestamp}.json"
        with open(raw_file, 'w', encoding='utf-8') as f:
            json.dump(raw_results, f, ensure_ascii=False, indent=2)
        
        print(f"çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {self.output_path}")
        print(f"- ãƒ¬ãƒãƒ¼ãƒˆ: {report_file.name}")
        print(f"- JSONãƒ¬ãƒãƒ¼ãƒˆ: {json_file.name}")
        print(f"- è©³ç´°çµæœ: {csv_file.name}")
        print(f"- ç”Ÿãƒ‡ãƒ¼ã‚¿: {raw_file.name}")
    
    async def _save_checkpoint(self):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        self.progress.save_checkpoint(self.checkpoint_path)
    
    async def _save_intermediate_results(self, results: List[DebateResult], batch_num: int):
        """ä¸­é–“çµæœã®ä¿å­˜"""
        intermediate_file = self.output_path / f"intermediate_batch_{batch_num}.json"
        
        results_data = [asdict(result) for result in results]
        with open(intermediate_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
    
    async def _load_existing_results(self) -> BenchmarkReport:
        """æ—¢å­˜ã®çµæœã‚’èª­ã¿è¾¼ã¿"""
        # æœ€æ–°ã®ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        json_files = list(self.output_path.glob("benchmark_report_*.json"))
        if not json_files:
            raise FileNotFoundError("æ—¢å­˜ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        latest_file = max(json_files, key=lambda x: x.stat().st_mtime)
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            report_data = json.load(f)
        
        # BenchmarkReportã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¾©å…ƒ
        # ç°¡ç•¥åŒ–ã®ãŸã‚ã€ä¸»è¦ãªçµ±è¨ˆã®ã¿è¡¨ç¤º
        print(f"æ—¢å­˜çµæœã‚’èª­ã¿è¾¼ã¿: {latest_file.name}")
        print(f"å…¨ä½“æ­£ç­”ç‡: {report_data['overall_accuracy']:.2%}")
        
        return report_data  # å®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ã«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå¾©å…ƒ
    
    async def _combine_results(
        self, 
        new_results: List[DebateResult], 
        all_problems: List[MMLUProblem]
    ) -> List[DebateResult]:
        """æ–°ã—ã„çµæœã¨æ—¢å­˜çµæœã‚’çµåˆ"""
        # ç°¡ç•¥åŒ–å®Ÿè£…ï¼šå®Ÿéš›ã¯ã‚ˆã‚Šè¤‡é›‘ãªçµåˆå‡¦ç†ãŒå¿…è¦
        return new_results
    
    def get_progress_summary(self) -> Dict[str, Any]:
        """é€²æ—ã‚µãƒãƒªãƒ¼ã®å–å¾—"""
        total_problems = self.config.questions_per_category * 4  # 4ã‚«ãƒ†ã‚´ãƒªæƒ³å®š
        completed = len(self.progress.completed_questions)
        failed = len(self.progress.failed_questions)
        
        return {
            "total_problems": total_problems,
            "completed": completed,
            "failed": failed,
            "remaining": total_problems - completed - failed,
            "completion_rate": completed / total_problems if total_problems > 0 else 0.0,
            "elapsed_time": time.time() - self.progress.start_time if self.progress.start_time else 0.0
        }


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    import asyncio
    
    async def test_runner():
        config = BenchmarkConfig(
            dataset_path="../../data/mmlu_pro_100.csv",
            output_dir="./test_results",
            max_turns_per_question=10,
            questions_per_category=2,  # ãƒ†ã‚¹ãƒˆç”¨ã«å°‘ãªãè¨­å®š
            batch_size=2,
            max_concurrent=1
        )
        
        runner = MMLUBenchmarkRunner(config)
        
        try:
            report = await runner.run_full_benchmark()
            print(f"ãƒ†ã‚¹ãƒˆå®Œäº†: æ­£ç­”ç‡ {report.overall_accuracy:.2%}")
        except Exception as e:
            print(f"ãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
    
    # asyncio.run(test_runner())