# MMLU PRO ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆæ›¸

## 1. ç›®æ¨™

### 1.1 ä¸»è¦ç›®æ¨™
- **å­¦è¡“çš„è©•ä¾¡ã®å®Ÿç¾**: æ—¢å­˜ã®ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè­°è«–ã‚·ã‚¹ãƒ†ãƒ ã‚’MMLU PROãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å®¢è¦³çš„ã«è©•ä¾¡
- **çŸ¥è­˜æ¨è«–èƒ½åŠ›ã®æ¸¬å®š**: è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å”èª¿ã«ã‚ˆã‚‹å•é¡Œè§£æ±ºèƒ½åŠ›ã®å®šé‡åŒ–
- **è­°è«–å“è³ªã®è©•ä¾¡**: å˜ç´”ãªæ­£ç­”ç‡ã ã‘ã§ãªãã€è­°è«–ãƒ—ãƒ­ã‚»ã‚¹ã®è³ªã‚‚æ¸¬å®š

### 1.2 è©•ä¾¡å¯¾è±¡
- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: MMLU PROï¼ˆå·¥å­¦ãƒ»ãã®ä»–ãƒ»ãƒ“ã‚¸ãƒã‚¹ãƒ»æ­´å²ã®4åˆ†é‡ã‹ã‚‰25å•ãšã¤ã€è¨ˆ100å•ï¼‰
- **è©•ä¾¡æŒ‡æ¨™**: 
  - æ­£ç­”ç‡ï¼ˆOverall Accuracyï¼‰
  - åˆ†é‡åˆ¥æ­£ç­”ç‡ï¼ˆDomain-specific Accuracyï¼‰
  - è­°è«–åŠ¹ç‡æ€§ï¼ˆTurn-to-Answer Ratioï¼‰
  - åˆæ„å½¢æˆåº¦ï¼ˆConsensus Scoreï¼‰

## 2. è¦ä»¶å®šç¾©

### 2.1 æ©Ÿèƒ½è¦ä»¶

#### 2.1.1 ãƒ‡ãƒ¼ã‚¿å‡¦ç†è¦ä»¶
- CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®MMLU PROå•é¡Œã®è‡ªå‹•èª­ã¿è¾¼ã¿
- æ—¥æœ¬èªå•é¡Œæ–‡ã®é©åˆ‡ãªå‡¦ç†ï¼ˆquestion_jaãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä½¿ç”¨ï¼‰
- é¸æŠè‚¢ã®å‹•çš„ãƒ‘ãƒ¼ã‚¹ï¼ˆé…åˆ—å½¢å¼ã¸ã®å¤‰æ›ï¼‰
- å•é¡ŒIDã«ã‚ˆã‚‹é€²æ—ç®¡ç†

#### 2.1.2 è­°è«–å®Ÿè¡Œè¦ä»¶
- å„å•é¡Œã‚’è­°è«–ãƒˆãƒ”ãƒƒã‚¯ã¨ã—ã¦è¨­å®š
- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒé¸æŠè‚¢ã‚’èªè­˜ã—ã¦è­°è«–
- æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ã®å‹•çš„èª¿æ•´ï¼ˆå•é¡Œã®è¤‡é›‘ã•ã«å¿œã˜ã¦ï¼‰
- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ©Ÿèƒ½ï¼ˆç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼‰

#### 2.1.3 å›ç­”æŠ½å‡ºè¦ä»¶
- æœ€çµ‚çµè«–ã‹ã‚‰é¸æŠè‚¢ï¼ˆA, B, C, Dç­‰ï¼‰ã®è‡ªå‹•æŠ½å‡º
- æ›–æ˜§ãªå›ç­”ã®å‡¦ç†ï¼ˆè¤‡æ•°å€™è£œã®å ´åˆï¼‰
- å›ç­”æœªæ±ºå®šã®å ´åˆã®å‡¦ç†

#### 2.1.4 è©•ä¾¡ãƒ»ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°è¦ä»¶
- æ­£ç­”ã¨ã®è‡ªå‹•æ¯”è¼ƒ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ã‚³ã‚¢æ›´æ–°
- åˆ†é‡åˆ¥é›†è¨ˆ
- è©³ç´°ãªè©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

### 2.2 éæ©Ÿèƒ½è¦ä»¶

#### 2.2.1 æ€§èƒ½è¦ä»¶
- 100å•ã®å®Ÿè¡Œæ™‚é–“: æœ€å¤§2æ™‚é–“ä»¥å†…
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 8GBä»¥ä¸‹
- APIå‘¼ã³å‡ºã—åˆ¶é™ã®è€ƒæ…®ï¼ˆOpenAI rate limitsï¼‰

#### 2.2.2 å¯ç”¨æ€§è¦ä»¶
- ã‚¨ãƒ©ãƒ¼æ™‚ã®è‡ªå‹•å¾©æ—§æ©Ÿèƒ½
- ä¸­æ–­ã‹ã‚‰ã®å†é–‹æ©Ÿèƒ½
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã®æ°¸ç¶šåŒ–

#### 2.2.3 ä¿å®ˆæ€§è¦ä»¶
- ãƒ­ã‚°å‡ºåŠ›ã®å……å®Ÿ
- è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤–éƒ¨åŒ–
- ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆã«ã‚ˆã‚‹æ‹¡å¼µæ€§

## 3. ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆ

### 3.1 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Benchmark Runner  â”‚
â”‚   (ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Problem Loader    â”‚
â”‚  (å•é¡Œãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Debate Orchestrator â”‚
â”‚   (è­°è«–ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Answer Extractor   â”‚
â”‚   (å›ç­”æŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Evaluation Engine  â”‚
â”‚    (è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Report Generator  â”‚
â”‚   (ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè¨­è¨ˆ

#### 3.2.1 Benchmark Runner
**è²¬å‹™**: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã®å…¨ä½“åˆ¶å¾¡
**ä¸»è¦æ©Ÿèƒ½**:
- å®Ÿè¡Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç®¡ç†
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¿½è·¡
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- çµæœé›†ç´„

```python
class BenchmarkRunner:
    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.progress = BenchmarkProgress()
        
    async def run_benchmark(self) -> BenchmarkResult:
        # å®Ÿè£…è©³ç´°ã¯å¾Œè¿°
        pass
```

#### 3.2.2 Problem Loader
**è²¬å‹™**: MMLU PROãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
**ä¸»ãªæ©Ÿèƒ½**:
- CSVè§£æ
- ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
- å•é¡Œã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°

```python
class ProblemLoader:
    def load_problems(self, csv_path: str) -> List[MMLUProblem]:
        # CSVã‹ã‚‰å•é¡Œã‚’èª­ã¿è¾¼ã¿ã€MMLUProblemã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        pass
    
    def validate_problem(self, problem: MMLUProblem) -> bool:
        # å•é¡Œãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        pass
```

#### 3.2.3 Answer Extractor
**è²¬å‹™**: è­°è«–çµè«–ã‹ã‚‰ã®é¸æŠè‚¢æŠ½å‡º
**æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯**:
1. **ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°**: ã€Œç­”ãˆã¯Aã€ã€Œé¸æŠè‚¢BãŒæ­£ã—ã„ã€ç­‰ã®è¡¨ç¾ã‚’æ¤œå‡º
2. **é¸æŠè‚¢ãƒ†ã‚­ã‚¹ãƒˆãƒãƒƒãƒãƒ³ã‚°**: çµè«–æ–‡ä¸­ã®é¸æŠè‚¢æœ¬æ–‡ã¨ã®é¡ä¼¼åº¦è¨ˆç®—
3. **LLMæ”¯æ´æŠ½å‡º**: å°‚ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã‚‹æ§‹é€ åŒ–æŠ½å‡º
4. **ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢**: æŠ½å‡ºçµæœã®ç¢ºä¿¡åº¦ã‚’0-1ã§æ•°å€¤åŒ–

```python
class AnswerExtractor:
    def extract_answer(self, conclusion: str, options: List[str]) -> AnswerExtraction:
        # è¤‡æ•°æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ãŸå›ç­”æŠ½å‡º
        pass
    
    def calculate_confidence(self, extraction_results: List[ExtractionResult]) -> float:
        # æŠ½å‡ºçµæœã®ä¿¡é ¼åº¦è¨ˆç®—
        pass
```

#### 3.2.4 Evaluation Engine
**è²¬å‹™**: è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—ã¨åˆ†æ
**è©•ä¾¡æŒ‡æ¨™**:

1. **åŸºæœ¬æŒ‡æ¨™**:
   - Overall Accuracy: å…¨ä½“æ­£ç­”ç‡
   - Domain Accuracy: åˆ†é‡åˆ¥æ­£ç­”ç‡
   - Confidence-weighted Accuracy: ä¿¡é ¼åº¦é‡ã¿ä»˜ãæ­£ç­”ç‡

2. **è­°è«–å“è³ªæŒ‡æ¨™**:
   - Average Turns per Question: å•é¡Œã‚ãŸã‚Šå¹³å‡ã‚¿ãƒ¼ãƒ³æ•°
   - Consensus Formation Time: åˆæ„å½¢æˆã¾ã§ã®æ™‚é–“
   - Question Utilization Rate: è³ªå•æ©Ÿèƒ½ã®æ´»ç”¨ç‡

3. **åŠ¹ç‡æ€§æŒ‡æ¨™**:
   - Time per Question: å•é¡Œã‚ãŸã‚Šå‡¦ç†æ™‚é–“
   - Token Usage: ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡
   - API Call Efficiency: APIå‘¼ã³å‡ºã—åŠ¹ç‡

### 3.3 ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«

#### 3.3.1 MMLUProblem
```python
@dataclass
class MMLUProblem:
    question_id: str
    question: str
    question_ja: str  # æ—¥æœ¬èªç‰ˆ
    options: List[str]
    correct_answer: str  # "A", "B", "C", etc.
    correct_index: int
    category: str
    source: str
    cot_content: str
```

#### 3.3.2 DebateResult
```python
@dataclass
class DebateResult:
    question_id: str
    final_conclusion: str
    full_transcript: List[str]
    turn_count: int
    debate_duration: float
    facilitator_interventions: int
    consensus_score: float
```

#### 3.3.3 AnswerExtraction
```python
@dataclass
class AnswerExtraction:
    extracted_answer: Optional[str]
    confidence_score: float
    extraction_method: str
    alternative_candidates: List[Tuple[str, float]]
    reasoning: str
```

#### 3.3.4 EvaluationResult
```python
@dataclass
class EvaluationResult:
    question_id: str
    predicted_answer: Optional[str]
    correct_answer: str
    is_correct: bool
    confidence_score: float
    debate_turns: int
    processing_time: float
    category: str
```

## 4. å®Ÿè£…æŒ‡é‡

### 4.1 è­°è«–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ”¹è‰¯

#### 4.1.1 å•é¡Œèªè­˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
ç¾åœ¨ã®æ±ç”¨è­°è«–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã€é¸æŠè‚¢å•é¡Œã«ç‰¹åŒ–ã—ã¦æ”¹è‰¯ï¼š

```python
MMLU_DEBATE_PROMPT = """
**å•é¡Œè§£æ±ºè­°è«–ã®ãƒ«ãƒ¼ãƒ«:**

**ç¾åœ¨ã®å•é¡Œ:**
{question_ja}

**é¸æŠè‚¢:**
{formatted_options}

**è­°è«–ã®é€²ã‚æ–¹:**
1. ã¾ãšå•é¡Œã‚’ç†è§£ã—ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’ç‰¹å®šã—ã¦ãã ã•ã„
2. å„é¸æŠè‚¢ã«ã¤ã„ã¦æ¤œè¨ã—ã€æ ¹æ‹ ã‚’ç¤ºã—ã¦è­°è«–ã—ã¦ãã ã•ã„
3. ä»–ã®å‚åŠ è€…ã®æ„è¦‹ã‚’èãã€å»ºè¨­çš„ã«è­°è«–ã‚’æ·±ã‚ã¦ãã ã•ã„
4. æœ€çµ‚çš„ã«ã€æœ€ã‚‚é©åˆ‡ã ã¨æ€ã†é¸æŠè‚¢ã‚’æ˜ç¢ºã«è¿°ã¹ã¦ãã ã•ã„

**é‡è¦**: æœ€çµ‚çµè«–ã§ã¯ã€Œç­”ãˆã¯â—‹â—‹ã§ã™ã€ã®å½¢ã§æ˜ç¢ºã«é¸æŠè‚¢ã‚’ç¤ºã—ã¦ãã ã•ã„ã€‚
"""
```

#### 4.1.2 çµè«–æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
```python
CONCLUSION_EXTRACTION_PROMPT = """
ä»¥ä¸‹ã®è­°è«–ã®çµè«–ã‚’åˆ†æã—ã€é¸æŠã•ã‚ŒãŸç­”ãˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

è­°è«–ã®çµè«–:
{conclusion}

é¸æŠè‚¢:
{options}

ä»¥ä¸‹ã®JSONå½¢å¼ã§ç­”ãˆã¦ãã ã•ã„:
{
    "selected_answer": "A/B/C/D/ç­‰",
    "confidence": 0.0-1.0,
    "reasoning": "æŠ½å‡ºæ ¹æ‹ "
}
"""
```

### 4.2 ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°æˆ¦ç•¥

#### 4.2.1 è­°è«–ã‚¨ãƒ©ãƒ¼å‡¦ç†
- **ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢**: æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ã®å¼·åˆ¶é©ç”¨
- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¿œç­”å¤±æ•—**: ä»£æ›¿ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ
- **APIåˆ¶é™å¯¾å¿œ**: æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã«ã‚ˆã‚‹ãƒªãƒˆãƒ©ã‚¤

#### 4.2.2 å›ç­”æŠ½å‡ºã‚¨ãƒ©ãƒ¼å‡¦ç†
- **æŠ½å‡ºå¤±æ•—æ™‚**: ã€Œä¸æ˜ã€ã¨ã—ã¦è¨˜éŒ²ã—ã€çµ±è¨ˆã«å«ã‚ã‚‹
- **è¤‡æ•°å€™è£œæ™‚**: æœ€é«˜ä¿¡é ¼åº¦ã®é¸æŠè‚¢ã‚’æ¡ç”¨
- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯**: äººé–“ã«ã‚ˆã‚‹æ‰‹å‹•ç¢ºèªã®ãŸã‚ã®ãƒ­ã‚°å‡ºåŠ›

### 4.3 å®Ÿè¡Œåˆ¶å¾¡

#### 4.3.1 ãƒãƒƒãƒå‡¦ç†è¨­è¨ˆ
```python
async def run_batch_evaluation(
    problems: List[MMLUProblem],
    batch_size: int = 10,
    max_concurrent: int = 3
) -> List[EvaluationResult]:
    """
    å•é¡Œã‚’ãƒãƒƒãƒå‡¦ç†ã§å®Ÿè¡Œ
    - APIåˆ¶é™ã‚’è€ƒæ…®ã—ãŸåŒæ™‚å®Ÿè¡Œæ•°åˆ¶å¾¡
    - ä¸­é–“çµæœã®å®šæœŸä¿å­˜
    - ã‚¨ãƒ©ãƒ¼æ™‚ã®è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤
    """
```

#### 4.3.2 ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç®¡ç†
```python
class BenchmarkProgress:
    def __init__(self):
        self.completed_questions = set()
        self.failed_questions = set()
        self.current_batch = 0
        self.start_time = None
        
    def save_checkpoint(self, filepath: str):
        """é€²æ—ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        
    def load_checkpoint(self, filepath: str):
        """ä¿å­˜ã•ã‚ŒãŸé€²æ—ã‚’èª­ã¿è¾¼ã¿"""
```

## 5. è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

### 5.1 çµ±è¨ˆåˆ†æ

#### 5.1.1 åŸºæœ¬çµ±è¨ˆ
- æ­£ç­”ç‡ã®95%ä¿¡é ¼åŒºé–“
- åˆ†é‡åˆ¥æ€§èƒ½æ¯”è¼ƒï¼ˆã‚«ã‚¤äºŒä¹—æ¤œå®šï¼‰
- è­°è«–ã‚¿ãƒ¼ãƒ³æ•°ã®åˆ†å¸ƒåˆ†æ

#### 5.1.2 ç›¸é–¢åˆ†æ
- è­°è«–ã‚¿ãƒ¼ãƒ³æ•°ã¨æ­£ç­”ç‡ã®ç›¸é–¢
- å•é¡Œé›£æ˜“åº¦ã¨åˆæ„å½¢æˆæ™‚é–“ã®é–¢ä¿‚
- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç™ºè¨€é‡ã¨æœ€çµ‚ç­”ãˆã®é–¢ä¿‚

### 5.2 å¯è¦–åŒ–

#### 5.2.1 æ€§èƒ½ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ­£ç­”ç‡ã‚°ãƒ©ãƒ•
- åˆ†é‡åˆ¥ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
- è­°è«–å“è³ªã‚¹ã‚³ã‚¢æ¨ç§»

#### 5.2.2 è©³ç´°åˆ†æãƒãƒ£ãƒ¼ãƒˆ
- å•é¡Œåˆ¥è­°è«–æ·±åº¦vsæ­£ç­”æ€§
- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç™ºè¨€ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
- ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹åˆ†é¡

## 6. å®Ÿè£…æ®µéš

### Phase 1: åŸºç›¤å®Ÿè£…ï¼ˆ1-2é€±é–“ï¼‰
1. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®å®Ÿè£…
2. åŸºæœ¬çš„ãªè­°è«–ã‚·ã‚¹ãƒ†ãƒ ã®æ”¹è‰¯
3. ç°¡å˜ãªå›ç­”æŠ½å‡ºæ©Ÿèƒ½

### Phase 2: è©•ä¾¡æ©Ÿèƒ½ï¼ˆ1é€±é–“ï¼‰
1. è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³ã®å®Ÿè£…
2. çµ±è¨ˆè¨ˆç®—æ©Ÿèƒ½
3. åŸºæœ¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

### Phase 3: æœ€é©åŒ–ãƒ»æ‹¡å¼µï¼ˆ1é€±é–“ï¼‰
1. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å¼·åŒ–
2. ä¸¦åˆ—å‡¦ç†ã®å®Ÿè£…
3. è©³ç´°åˆ†ææ©Ÿèƒ½

### Phase 4: æ¤œè¨¼ãƒ»èª¿æ•´ï¼ˆ1é€±é–“ï¼‰
1. å°è¦æ¨¡ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹èª¿æ•´
3. æœ€çµ‚æ¤œè¨¼

## 7. è¨­å®šä¾‹

### 7.1 benchmark_config.yaml
```yaml
benchmark:
  dataset_path: "data/mmlu_pro_100.csv"
  output_dir: "results/benchmark_run_{timestamp}"
  
  sampling:
    total_questions: 100
    questions_per_category: 25
    random_seed: 42
    
  debate_settings:
    max_turns_per_question: 15
    timeout_per_question: 300  # seconds
    facilitator_check_interval: 5
    
  evaluation:
    confidence_threshold: 0.7
    retry_failed_extractions: true
    save_intermediate_results: true
    
  performance:
    max_concurrent_debates: 3
    batch_size: 10
    api_delay: 1.0  # seconds between API calls
```

## 8. å®Ÿè£…ã®å…·ä½“çš„æŒ‡ç¤º

### 8.1 æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®æ”¹è‰¯ãƒã‚¤ãƒ³ãƒˆ

#### 8.1.1 config.pyã®æ‹¡å¼µ
```python
# MMLUå°‚ç”¨ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®šã‚’è¿½åŠ 
MMLU_AGENTS_CONFIG = [
    {
        "name": "è«–ç†åˆ†æè€…",
        "persona": "è«–ç†çš„æ€è€ƒã‚’é‡è¦–ã—ã€é¸æŠè‚¢ã‚’ä½“ç³»çš„ã«åˆ†æã™ã‚‹å°‚é–€å®¶ã€‚æ ¹æ‹ ã‚’æ˜ç¢ºã«ç¤ºã—ã¦åˆ¤æ–­ã™ã‚‹ã€‚",
        "avatar": "ğŸ”",
        "evaluation_focus": "logical_reasoning"
    },
    {
        "name": "çŸ¥è­˜çµ±åˆè€…", 
        "persona": "å¹…åºƒã„çŸ¥è­˜ã‚’çµ±åˆã—ã¦ç·åˆçš„ã«åˆ¤æ–­ã™ã‚‹å°‚é–€å®¶ã€‚ç•°ãªã‚‹è¦–ç‚¹ã‚’çµ„ã¿åˆã‚ã›ã¦çµè«–ã‚’å°ãã€‚",
        "avatar": "ğŸ“š",
        "evaluation_focus": "knowledge_integration"
    },
    {
        "name": "æ‰¹åˆ¤çš„æ¤œè¨¼è€…",
        "persona": "ä»–ã®æ„è¦‹ã‚’æ‰¹åˆ¤çš„ã«æ¤œè¨¼ã—ã€èª¤ã‚Šã‚„è¦‹è½ã¨ã—ã‚’æŒ‡æ‘˜ã™ã‚‹å°‚é–€å®¶ã€‚åå¯¾æ„è¦‹ã‚‚ç©æ¥µçš„ã«æç¤ºã™ã‚‹ã€‚",
        "avatar": "âš–ï¸", 
        "evaluation_focus": "critical_analysis"
    }
]
```

#### 8.1.2 state.pyã®æ‹¡å¼µ
```python
class MMLUConversationState(ConversationState):
    """MMLUç”¨ã®æ‹¡å¼µçŠ¶æ…‹"""
    # æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«åŠ ãˆã¦ä»¥ä¸‹ã‚’è¿½åŠ 
    current_problem: Optional[MMLUProblem]
    extracted_answers: List[str]  # å„ã‚¿ãƒ¼ãƒ³ã§æŠ½å‡ºã•ã‚ŒãŸå›ç­”å€™è£œ
    answer_confidences: List[float]  # å›ç­”ã®ä¿¡é ¼åº¦å±¥æ­´
    option_mentions: Dict[str, int]  # å„é¸æŠè‚¢ã®è¨€åŠå›æ•°
    reasoning_quality: float  # æ¨è«–ã®è³ªçš„è©•ä¾¡
```

### 8.2 æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆæŒ‡ç¤º

#### 8.2.1 src/mmlu_benchmark/data_loader.py
```python
"""
MMLU PROãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
- CSVè§£æã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
- å•é¡Œã®å‰å‡¦ç†
- ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
"""

class MMLUDataLoader:
    def __init__(self, csv_path: str):
        self.csv_path = csv_path
        
    def load_and_validate(self) -> List[MMLUProblem]:
        """CSVã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼ã—ã¦è¿”ã™"""
        
    def stratified_sample(self, n_per_category: int = 25) -> List[MMLUProblem]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«å‡ç­‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        
    def preprocess_options(self, options_str: str) -> List[str]:
        """é¸æŠè‚¢æ–‡å­—åˆ—ã‚’é…åˆ—ã«å¤‰æ›"""
```

#### 8.2.2 src/mmlu_benchmark/answer_extractor.py
```python
"""
è­°è«–çµè«–ã‹ã‚‰ã®å›ç­”æŠ½å‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
- ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
- æ„å‘³çš„é¡ä¼¼åº¦è¨ˆç®—
- LLMæ”¯æ´æŠ½å‡º
"""

class AnswerExtractor:
    def __init__(self):
        self.extraction_strategies = [
            PatternMatchingStrategy(),
            SemanticSimilarityStrategy(), 
            LLMAssistedStrategy()
        ]
        
    def extract_with_confidence(
        self, 
        conclusion: str, 
        options: List[str]
    ) -> AnswerExtraction:
        """è¤‡æ•°æ‰‹æ³•ã§å›ç­”ã‚’æŠ½å‡ºã—ã€ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
```

#### 8.2.3 src/mmlu_benchmark/benchmark_runner.py
```python
"""
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã®ãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
- å•é¡Œã®é †æ¬¡å®Ÿè¡Œ
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç®¡ç†
- çµæœé›†è¨ˆ
"""

class MMLUBenchmarkRunner:
    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.data_loader = MMLUDataLoader(config.dataset_path)
        self.evaluator = MMLUEvaluator()
        
    async def run_full_benchmark(self) -> BenchmarkReport:
        """100å•ã®å®Œå…¨å®Ÿè¡Œ"""
        
    async def run_single_problem(self, problem: MMLUProblem) -> EvaluationResult:
        """å˜ä¸€å•é¡Œã®å®Ÿè¡Œã¨è©•ä¾¡"""
```

### 8.3 è­°è«–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…·ä½“çš„æ”¹è‰¯

#### 8.3.1 agents.pyã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿®æ­£
ç¾åœ¨ã®PROMPT_TEMPLATE_STRã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«å¤‰æ›´ï¼š

```python
MMLU_PROMPT_TEMPLATE_STR = """
**ã‚ãªãŸã®æƒ…å ±:**
{persona}

**ç¾åœ¨è§£æ±ºã™ã¹ãå•é¡Œ:**
{question_ja}

**é¸æŠè‚¢:**
{formatted_options}

**è­°è«–ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³:**
1. **å•é¡Œç†è§£**: ã¾ãšå•é¡Œã®æ ¸å¿ƒã‚’æ­£ç¢ºã«æŠŠæ¡ã—ã¦ãã ã•ã„
2. **é¸æŠè‚¢åˆ†æ**: å„é¸æŠè‚¢ã«ã¤ã„ã¦å…·ä½“çš„ãªæ ¹æ‹ ã¨ã¨ã‚‚ã«è©•ä¾¡ã—ã¦ãã ã•ã„
3. **çŸ¥è­˜æ´»ç”¨**: ã‚ãªãŸã®å°‚é–€çŸ¥è­˜ã‚’æ´»ç”¨ã—ã¦åˆ¤æ–­ã—ã¦ãã ã•ã„
4. **å»ºè¨­çš„è­°è«–**: ä»–ã®å‚åŠ è€…ã®æ„è¦‹ã‚’èãã€å»ºè¨­çš„ã«è­°è«–ã‚’æ·±ã‚ã¦ãã ã•ã„
5. **æ˜ç¢ºãªçµè«–**: æœ€çµ‚çš„ã«ã€Œç­”ãˆã¯[é¸æŠè‚¢]ã§ã™ã€ã®å½¢ã§æ˜ç¢ºã«è¿°ã¹ã¦ãã ã•ã„

**é‡è¦ãªåˆ¶ç´„:**
- å¿…ãšé¸æŠè‚¢Aã€Bã€Cã€Dã€Eã€Fã€Gã€Hã€Iã€Jã®ä¸­ã‹ã‚‰ä¸€ã¤ã‚’é¸ã‚“ã§ãã ã•ã„
- ã€Œåˆ†ã‹ã‚‰ãªã„ã€ã‚„ã€Œåˆ¤æ–­ã§ããªã„ã€ã¯é¿ã‘ã¦ãã ã•ã„
- æ ¹æ‹ ã‚’ç¤ºã—ã¦è«–ç†çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„

ç¾åœ¨ã®è­°è«–ãƒ†ãƒ¼ãƒ: {question_ja}
"""
```

### 8.4 è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…æŒ‡ç¤º

#### 8.4.1 evaluation/evaluator.py
```python
class MMLUEvaluator:
    """MMLUå°‚ç”¨è©•ä¾¡å™¨"""
    
    def __init__(self):
        self.answer_extractor = AnswerExtractor()
        self.metrics_calculator = MetricsCalculator()
        
    def evaluate_single_result(
        self, 
        debate_result: DebateResult,
        problem: MMLUProblem
    ) -> EvaluationResult:
        """å˜ä¸€å•é¡Œã®è©•ä¾¡"""
        
        # 1. å›ç­”æŠ½å‡º
        extraction = self.answer_extractor.extract_with_confidence(
            debate_result.final_conclusion,
            problem.options
        )
        
        # 2. æ­£ç­”åˆ¤å®š
        is_correct = (extraction.extracted_answer == problem.correct_answer)
        
        # 3. è­°è«–å“è³ªè©•ä¾¡
        quality_metrics = self._evaluate_debate_quality(debate_result)
        
        return EvaluationResult(
            question_id=problem.question_id,
            predicted_answer=extraction.extracted_answer,
            correct_answer=problem.correct_answer,
            is_correct=is_correct,
            confidence_score=extraction.confidence_score,
            debate_turns=debate_result.turn_count,
            processing_time=debate_result.debate_duration,
            category=problem.category,
            quality_metrics=quality_metrics
        )
```

### 8.5 å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆæŒ‡ç¤º

#### 8.5.1 run_mmlu_benchmark.py
```python
#!/usr/bin/env python3
"""
MMLU PROãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import asyncio
import argparse
from pathlib import Path
from src.mmlu_benchmark.benchmark_runner import MMLUBenchmarkRunner
from src.mmlu_benchmark.config import BenchmarkConfig

async def main():
    parser = argparse.ArgumentParser(description='Run MMLU PRO Benchmark')
    parser.add_argument('--config', default='config/benchmark_config.yaml')
    parser.add_argument('--dataset', default='data/mmlu_pro_100.csv')
    parser.add_argument('--output-dir', default='results/')
    parser.add_argument('--resume', action='store_true', help='Resume from checkpoint')
    
    args = parser.parse_args()
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = BenchmarkConfig.from_yaml(args.config)
    config.dataset_path = args.dataset
    config.output_dir = args.output_dir
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    runner = MMLUBenchmarkRunner(config)
    
    try:
        if args.resume:
            report = await runner.resume_benchmark()
        else:
            report = await runner.run_full_benchmark()
            
        print(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†!")
        print(f"å…¨ä½“æ­£ç­”ç‡: {report.overall_accuracy:.2%}")
        print(f"çµæœä¿å­˜å…ˆ: {report.output_path}")
        
    except KeyboardInterrupt:
        print("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ä¸­æ–­ - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ä¸­...")
        await runner.save_checkpoint()
        
if __name__ == "__main__":
    asyncio.run(main())
```

### 8.6 çµæœåˆ†æãƒ„ãƒ¼ãƒ«ã®æŒ‡ç¤º

#### 8.6.1 analysis/result_analyzer.py
```python
class BenchmarkAnalyzer:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®è©³ç´°åˆ†æ"""
    
    def generate_comprehensive_report(self, results: List[EvaluationResult]) -> str:
        """åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        report_sections = [
            self._generate_summary_stats(results),
            self._generate_category_analysis(results), 
            self._generate_error_analysis(results),
            self._generate_debate_quality_analysis(results),
            self._generate_recommendations(results)
        ]
        
        return "\n\n".join(report_sections)
    
    def create_visualizations(self, results: List[EvaluationResult], output_dir: Path):
        """å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ã®ç”Ÿæˆ"""
        # matplotlib/seabornã‚’ä½¿ç”¨ã—ãŸå¯è¦–åŒ–
        pass
```

### 8.7 ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

#### 8.7.1 å˜ä½“ãƒ†ã‚¹ãƒˆ
```python
# tests/test_answer_extractor.py
def test_answer_extraction_patterns():
    """å›ç­”æŠ½å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ†ã‚¹ãƒˆ"""
    
def test_confidence_calculation():
    """ä¿¡é ¼åº¦è¨ˆç®—ã®ãƒ†ã‚¹ãƒˆ"""

# tests/test_mmlu_loader.py  
def test_csv_loading():
    """CSVèª­ã¿è¾¼ã¿ã®ãƒ†ã‚¹ãƒˆ"""
    
def test_stratified_sampling():
    """å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ"""
```

#### 8.7.2 çµ±åˆãƒ†ã‚¹ãƒˆ
```python
# tests/integration/test_benchmark_flow.py
async def test_end_to_end_benchmark():
    """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆï¼ˆ5å•ç¨‹åº¦ã§å®Ÿè¡Œï¼‰"""
```

### 8.8 å®Ÿè£…ã®å„ªå…ˆé †ä½

#### é«˜å„ªå…ˆåº¦ï¼ˆå¿…é ˆæ©Ÿèƒ½ï¼‰
1. `MMLUDataLoader`ã®å®Ÿè£…
2. æ—¢å­˜ã®`orchestrator.py`ã®MMLUå¯¾å¿œæ”¹è‰¯
3. `AnswerExtractor`ã®åŸºæœ¬å®Ÿè£…
4. è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…

#### ä¸­å„ªå…ˆåº¦ï¼ˆå“è³ªå‘ä¸Šï¼‰
1. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å¼·åŒ–
2. ä¸¦åˆ—å‡¦ç†ã®å®Ÿè£…
3. è©³ç´°åˆ†ææ©Ÿèƒ½
4. å¯è¦–åŒ–æ©Ÿèƒ½

#### ä½å„ªå…ˆåº¦ï¼ˆæ‹¡å¼µæ©Ÿèƒ½ï¼‰
1. Web UIã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
2. ä»–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¸ã®æ‹¡å¼µ
3. é«˜åº¦ãªçµ±è¨ˆåˆ†ææ©Ÿèƒ½

ã“ã®è¨­è¨ˆã«åŸºã¥ã„ã¦å®Ÿè£…ã™ã‚‹ã“ã¨ã§ã€å­¦è¡“çš„ã«æœ‰æ„ç¾©ãªãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè­°è«–ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚å®Ÿè£…æ™‚ã¯é«˜å„ªå…ˆåº¦ã®æ©Ÿèƒ½ã‹ã‚‰é †ç•ªã«é€²ã‚ã€å„æ®µéšã§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦å“è³ªã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ã€‚