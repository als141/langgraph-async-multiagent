#!/usr/bin/env python3
"""
ã‚·ãƒ³ãƒ—ãƒ«ãªOpenAI GPT-4.1ã‚’ä½¿ç”¨ã—ãŸMMLUå•é¡Œã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ã§å›ç­”ã‚’å–å¾—
"""

import os
import json
import time
import asyncio
import re
from typing import List, Dict, Any
from dataclasses import dataclass
from pathlib import Path
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

from openai import OpenAI

@dataclass
class MMLUResult:
    """MMLUå•é¡Œã®çµæœ"""
    question_id: str
    question: str
    options: List[str]
    correct_answer: str
    predicted_answer: str
    is_correct: bool
    response_time: float

class SimpleMMLUScorer:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªMMLUã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        self.client = OpenAI(api_key=api_key)
        
        # JSONã‚¹ã‚­ãƒ¼ãƒå®šç¾©
        self.json_schema = {
            "type": "object",
            "properties": {
                "final_answer": {
                    "type": "string",
                    "description": "é¸æŠã—ãŸé¸æŠè‚¢ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã®ã¿ï¼ˆA, B, C, D, E, F, G, H, I, Jã®ã„ãšã‚Œã‹ï¼‰",
                    "enum": ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J"]
                }
            },
            "required": ["final_answer"],
            "additionalProperties": False
        }
    
    def load_csv_data(self, csv_path: str) -> List[Dict[str, Any]]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        import pandas as pd
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
        
        df = pd.read_csv(csv_path)
        problems = []
        
        for _, row in df.iterrows():
            try:
                # é¸æŠè‚¢ã®è§£æ
                options_str = row['options']
                options = self._parse_options(options_str)
                
                problem = {
                    'question_id': str(row['question_id']),
                    'question': str(row['question_ja']),  # æ—¥æœ¬èªç‰ˆã‚’ä½¿ç”¨
                    'options': options,
                    'correct_answer': str(row['answer']),
                    'category': str(row['category'])
                }
                problems.append(problem)
                
            except Exception as e:
                print(f"å•é¡Œ {row.get('question_id', 'unknown')} ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"âœ… {len(problems)}å•ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        return problems
    
    def _parse_options(self, options_str: str) -> List[str]:
        """é¸æŠè‚¢æ–‡å­—åˆ—ã‚’è§£æ"""
        import json
        import re
        
        try:
            # JSONå½¢å¼ã§ã®è§£æã‚’è©¦è¡Œ
            if options_str.startswith('[') and options_str.endswith(']'):
                options = json.loads(options_str)
                return [str(option).strip() for option in options]
        except:
            pass
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        return []
    
    def create_user_prompt(self, question: str, options: List[str]) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ"""
        # é¸æŠè‚¢ã‚’æ•´å½¢
        formatted_options = []
        for i, option in enumerate(options):
            letter = chr(ord('A') + i)
            formatted_options.append(f"{letter}) {option}")
        
        prompt = f"""å¿…ãš A, B, C, D, E, F, G, H, I, J ã®ã„ãšã‚Œã‹ã‚’é¸æŠã—ã¦ãã ã•ã„
æœ€é©ãªé¸æŠè‚¢ã‚’æ±ºå®šã—ã¦ãã ã•ã„

å•é¡Œ:
{question}

é¸æŠè‚¢:
{chr(10).join(formatted_options)}

å›ç­”ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§è¡Œã£ã¦ãã ã•ã„:
{{
"final_answer": "é¸æŠã—ãŸé¸æŠè‚¢ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã®ã¿"
}}"""
        
        return prompt
    
    def get_answer(self, question: str, options: List[str]) -> Dict[str, Any]:
        """å˜ä¸€å•é¡Œã®å›ç­”ã‚’å–å¾—"""
        user_prompt = self.create_user_prompt(question, options)
        
        start_time = time.time()
        
        try:
            response = self.client.responses.create(
                model="o3-mini",
                input=[{"role": "user", "content": user_prompt}],
                text={
                    "format": {
                        "type": "json_schema",
                        "name": "mmlu_answer",
                        "schema": self.json_schema,
                        "strict": True
                    }
                }
            )
            
            response_time = time.time() - start_time
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰å›ç­”ã‚’æŠ½å‡º
            if response.status == "completed":
                output_text = response.output_text
                try:
                    parsed_response = json.loads(output_text)
                    predicted_answer = parsed_response.get("final_answer", "")
                    
                    return {
                        "predicted_answer": predicted_answer,
                        "response_time": response_time,
                        "success": True,
                        "raw_response": output_text
                    }
                except json.JSONDecodeError:
                    return {
                        "predicted_answer": "",
                        "response_time": response_time,
                        "success": False,
                        "error": "JSONè§£æã‚¨ãƒ©ãƒ¼",
                        "raw_response": output_text
                    }
            else:
                return {
                    "predicted_answer": "",
                    "response_time": response_time,
                    "success": False,
                    "error": f"APIå‘¼ã³å‡ºã—å¤±æ•—: {response.status}",
                    "raw_response": str(response)
                }
        
        except Exception as e:
            response_time = time.time() - start_time
            return {
                "predicted_answer": "",
                "response_time": response_time,
                "success": False,
                "error": str(e)
            }
    
    async def get_answer_async(self, question: str, options: List[str]) -> Dict[str, Any]:
        """éåŒæœŸã§å˜ä¸€å•é¡Œã®å›ç­”ã‚’å–å¾—"""
        user_prompt = self.create_user_prompt(question, options)
        
        start_time = time.time()
        
        try:
            # éåŒæœŸã§APIå‘¼ã³å‡ºã—ï¼ˆå®Ÿéš›ã«ã¯OpenAI Python SDKã¯åŒæœŸã®ã¿ãªã®ã§ã€run_in_executorã‚’ä½¿ç”¨ï¼‰
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, 
                lambda: self.client.responses.create(
                    model="o3-mini",
                    input=[{"role": "user", "content": user_prompt}],
                    text={
                        "format": {
                            "type": "json_schema",
                            "name": "mmlu_answer",
                            "schema": self.json_schema,
                            "strict": True
                        }
                    }
                )
            )
            
            response_time = time.time() - start_time
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰å›ç­”ã‚’æŠ½å‡º
            if response.status == "completed":
                output_text = response.output_text
                try:
                    parsed_response = json.loads(output_text)
                    predicted_answer = parsed_response.get("final_answer", "")
                    
                    return {
                        "predicted_answer": predicted_answer,
                        "response_time": response_time,
                        "success": True,
                        "raw_response": output_text
                    }
                except json.JSONDecodeError:
                    return {
                        "predicted_answer": "",
                        "response_time": response_time,
                        "success": False,
                        "error": "JSONè§£æã‚¨ãƒ©ãƒ¼",
                        "raw_response": output_text
                    }
            else:
                return {
                    "predicted_answer": "",
                    "response_time": response_time,
                    "success": False,
                    "error": f"APIå‘¼ã³å‡ºã—å¤±æ•—: {response.status}",
                    "raw_response": str(response)
                }
        
        except Exception as e:
            response_time = time.time() - start_time
            return {
                "predicted_answer": "",
                "response_time": response_time,
                "success": False,
                "error": str(e)
            }
    
    async def process_batch(self, problems: List[Dict[str, Any]], batch_size: int = 5) -> List[MMLUResult]:
        """ãƒãƒƒãƒå‡¦ç†ã§å•é¡Œã‚’ä¸¦è¡Œå‡¦ç†"""
        results = []
        
        # ãƒãƒƒãƒã«åˆ†å‰²
        for i in range(0, len(problems), batch_size):
            batch = problems[i:i + batch_size]
            batch_num = i // batch_size + 1
            total_batches = (len(problems) - 1) // batch_size + 1
            
            print(f"\nğŸ”„ ãƒãƒƒãƒ {batch_num}/{total_batches} ({len(batch)}å•) ã‚’ä¸¦è¡Œå‡¦ç†ä¸­...")
            
            # ãƒãƒƒãƒå†…ã®å•é¡Œã‚’ä¸¦è¡Œå®Ÿè¡Œ
            batch_tasks = []
            for problem in batch:
                task = self.get_answer_async(problem['question'], problem['options'])
                batch_tasks.append((problem, task))
            
            # ä¸¦è¡Œå®Ÿè¡Œ
            batch_results = await asyncio.gather(*[task for _, task in batch_tasks], return_exceptions=True)
            
            # çµæœå‡¦ç†
            for j, ((problem, _), answer_result) in enumerate(zip(batch_tasks, batch_results)):
                if isinstance(answer_result, Exception):
                    print(f"  âŒ å•é¡Œ {problem['question_id']}: ã‚¨ãƒ©ãƒ¼ - {answer_result}")
                    answer_result = {
                        "predicted_answer": "",
                        "response_time": 0.0,
                        "success": False,
                        "error": str(answer_result)
                    }
                
                # æ­£ç­”åˆ¤å®š
                is_correct = (answer_result['predicted_answer'] == problem['correct_answer'])
                
                # çµæœè¨˜éŒ²
                result = MMLUResult(
                    question_id=problem['question_id'],
                    question=problem['question'],
                    options=problem['options'],
                    correct_answer=problem['correct_answer'],
                    predicted_answer=answer_result['predicted_answer'],
                    is_correct=is_correct,
                    response_time=answer_result['response_time']
                )
                results.append(result)
                
                # é€²æ—è¡¨ç¤º
                status = "âœ…" if is_correct else "âŒ"
                print(f"  {status} å•é¡Œ {problem['question_id']}: "
                      f"äºˆæ¸¬={answer_result['predicted_answer']}, æ­£è§£={problem['correct_answer']}")
                
                # ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
                if not answer_result['success']:
                    print(f"    âš ï¸ ã‚¨ãƒ©ãƒ¼: {answer_result.get('error', 'Unknown error')}")
            
            # ãƒãƒƒãƒé–“ã®å¾…æ©Ÿï¼ˆAPIåˆ¶é™å¯¾ç­–ï¼‰
            if i + batch_size < len(problems):
                print(f"  â³ æ¬¡ã®ãƒãƒƒãƒã¾ã§1ç§’å¾…æ©Ÿ...")
                await asyncio.sleep(1)
        
        return results
    
    def score_all_problems(self, csv_path: str, output_path: str = None, batch_size: int = 1) -> Dict[str, Any]:
        """å…¨å•é¡Œã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆåŒæœŸç‰ˆãƒ»äº’æ›æ€§ã®ãŸã‚ï¼‰"""
        return asyncio.run(self.score_all_problems_async(csv_path, output_path, batch_size))
    
    async def score_all_problems_async(self, csv_path: str, output_path: str = None, batch_size: int = 5) -> Dict[str, Any]:
        """å…¨å•é¡Œã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆéåŒæœŸç‰ˆï¼‰"""
        problems = self.load_csv_data(csv_path)
        
        print(f"ğŸš€ {len(problems)}å•ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚’é–‹å§‹... (ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size})")
        
        # ãƒãƒƒãƒå‡¦ç†ã§å®Ÿè¡Œ
        if batch_size > 1:
            results = await self.process_batch(problems, batch_size)
        else:
            # ãƒãƒƒãƒã‚µã‚¤ã‚º1ã®å ´åˆã¯é †æ¬¡å‡¦ç†
            results = []
            for i, problem in enumerate(problems, 1):
                print(f"[{i}/{len(problems)}] å•é¡Œ {problem['question_id']} ã‚’å‡¦ç†ä¸­...")
                
                # å›ç­”å–å¾—
                answer_result = self.get_answer(problem['question'], problem['options'])
                
                # æ­£ç­”åˆ¤å®š
                is_correct = (answer_result['predicted_answer'] == problem['correct_answer'])
                
                # çµæœè¨˜éŒ²
                result = MMLUResult(
                    question_id=problem['question_id'],
                    question=problem['question'],
                    options=problem['options'],
                    correct_answer=problem['correct_answer'],
                    predicted_answer=answer_result['predicted_answer'],
                    is_correct=is_correct,
                    response_time=answer_result['response_time']
                )
                results.append(result)
                
                # é€²æ—è¡¨ç¤º
                accuracy = sum(1 for r in results if r.is_correct) / len(results)
                status = "âœ…" if is_correct else "âŒ"
                print(f"  {status} äºˆæ¸¬: {answer_result['predicted_answer']}, æ­£è§£: {problem['correct_answer']} "
                      f"(ç¾åœ¨ã®æ­£ç­”ç‡: {accuracy:.1%})")
                
                # ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
                if not answer_result['success']:
                    print(f"  âš ï¸ ã‚¨ãƒ©ãƒ¼: {answer_result.get('error', 'Unknown error')}")
        
        # æœ€çµ‚çµæœé›†è¨ˆ
        correct_count = sum(1 for result in results if result.is_correct)
        overall_accuracy = correct_count / len(results) if results else 0
        
        summary = {
            "total_questions": len(problems),
            "correct_answers": correct_count,
            "overall_accuracy": overall_accuracy,
            "results": results,
            "batch_size": batch_size
        }
        
        # çµæœä¿å­˜
        if output_path:
            self._save_results(summary, output_path)
        
        return summary
    
    def _save_results(self, summary: Dict[str, Any], output_path: str):
        """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        output_dir = Path(output_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # çµæœã‚’JSONå½¢å¼ã§ä¿å­˜
        save_data = {
            "total_questions": summary["total_questions"],
            "correct_answers": summary["correct_answers"],
            "overall_accuracy": summary["overall_accuracy"],
            "results": []
        }
        
        for result in summary["results"]:
            save_data["results"].append({
                "question_id": result.question_id,
                "question": result.question,
                "options": result.options,
                "correct_answer": result.correct_answer,
                "predicted_answer": result.predicted_answer,
                "is_correct": result.is_correct,
                "response_time": result.response_time
            })
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ã‚·ãƒ³ãƒ—ãƒ«ãªMMLUã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--csv', default='data/mmlu_pro_100.csv', help='CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--output', default='results/simple_scoring_results.json', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--limit', type=int, help='å‡¦ç†ã™ã‚‹å•é¡Œæ•°ã®ä¸Šé™')
    parser.add_argument('--batch-size', type=int, default=1, help='ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆä¸¦è¡Œå‡¦ç†æ•°ï¼‰')
    
    args = parser.parse_args()
    
    try:
        scorer = SimpleMMLUScorer()
        
        # CSVèª­ã¿è¾¼ã¿
        problems = scorer.load_csv_data(args.csv)
        
        # å•é¡Œæ•°åˆ¶é™
        if args.limit and args.limit < len(problems):
            problems = problems[:args.limit]
            print(f"âš ï¸ å•é¡Œæ•°ã‚’{args.limit}å•ã«åˆ¶é™ã—ã¾ã—ãŸ")
        
        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å®Ÿè¡Œ
        results = scorer.score_all_problems(args.csv, args.output, args.batch_size)
        
        # æœ€çµ‚çµæœè¡¨ç¤º
        print("\n" + "="*60)
        print("ğŸ‰ ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å®Œäº†!")
        print("="*60)
        print(f"ğŸ“Š ç·å•é¡Œæ•°: {results['total_questions']}")
        print(f"âœ… æ­£ç­”æ•°: {results['correct_answers']}")
        print(f"ğŸ“ˆ å…¨ä½“æ­£ç­”ç‡: {results['overall_accuracy']:.1%}")
        print(f"âš¡ ãƒãƒƒãƒã‚µã‚¤ã‚º: {results['batch_size']}")
        print(f"ğŸ“ çµæœä¿å­˜å…ˆ: {args.output}")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())