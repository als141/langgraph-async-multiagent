#!/usr/bin/env python3
"""
MMLU PROãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
import asyncio
import argparse
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.score.mmlu_benchmark_runner import MMLUBenchmarkRunner, BenchmarkConfig


def create_output_directory(base_dir: str = "results") -> str:
    """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(base_dir) / f"mmlu_benchmark_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    return str(output_dir)


def print_banner():
    """ãƒãƒŠãƒ¼è¡¨ç¤º"""
    print("="*80)
    print("ğŸ¯ MMLU PRO ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè­°è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("="*80)
    print()


def print_config_summary(config: BenchmarkConfig):
    """è¨­å®šã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
    print("ğŸ“‹ å®Ÿè¡Œè¨­å®š:")
    print(f"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {config.dataset_path}")
    print(f"  å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {config.output_dir}")
    print(f"  ã‚«ãƒ†ã‚´ãƒªã‚ãŸã‚Šå•é¡Œæ•°: {config.questions_per_category}")
    print(f"  æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°: {config.max_turns_per_question}")
    print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}")
    print(f"  åŒæ™‚å®Ÿè¡Œæ•°: {config.max_concurrent}")
    print(f"  ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {config.timeout_per_question}ç§’")
    print()


async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='MMLU PROãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # å…¨100å•å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
  python run_mmlu_benchmark.py
  
  # å„ã‚«ãƒ†ã‚´ãƒª10å•ãšã¤å®Ÿè¡Œ
  python run_mmlu_benchmark.py --questions-per-category 10
  
  # è­°è«–ã‚¿ãƒ¼ãƒ³æ•°ã‚’åˆ¶é™
  python run_mmlu_benchmark.py --max-turns 10
  
  # ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨
  python run_mmlu_benchmark.py --dataset custom_data.csv
  
  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹
  python run_mmlu_benchmark.py --resume --output-dir results/mmlu_benchmark_20240101_120000
        """
    )
    
    # å¼•æ•°å®šç¾©
    parser.add_argument(
        '--dataset',
        default='data/mmlu_pro_100.csv',
        help='ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: data/mmlu_pro_100.csv)'
    )
    
    parser.add_argument(
        '--output-dir',
        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: è‡ªå‹•ç”Ÿæˆ)'
    )
    
    parser.add_argument(
        '--questions-per-category',
        type=int,
        default=25,
        help='ã‚«ãƒ†ã‚´ãƒªã‚ãŸã‚Šå•é¡Œæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 25)'
    )
    
    parser.add_argument(
        '--max-turns',
        type=int,
        default=15,
        help='å•é¡Œã‚ãŸã‚Šæœ€å¤§ã‚¿ãƒ¼ãƒ³æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 15)'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=10,
        help='ãƒãƒƒãƒã‚µã‚¤ã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10)'
    )
    
    parser.add_argument(
        '--max-concurrent',
        type=int,
        default=1,
        help='åŒæ™‚å®Ÿè¡Œæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ã€APIåˆ¶é™ã«æ³¨æ„)'
    )
    
    parser.add_argument(
        '--timeout',
        type=int,
        default=300,
        help='å•é¡Œã‚ãŸã‚Šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300)'
    )
    
    parser.add_argument(
        '--resume',
        action='store_true',
        help='ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹'
    )
    
    parser.add_argument(
        '--test-mode',
        action='store_true',
        help='ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆå„ã‚«ãƒ†ã‚´ãƒª2å•ãšã¤ï¼‰'
    )
    
    parser.add_argument(
        '--experiment-mode',
        type=int,
        metavar='N',
        help='å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ï¼ˆå„ã‚«ãƒ†ã‚´ãƒªNå•ãšã¤ã€ä¾‹: --experiment-mode 3ï¼‰'
    )
    
    parser.add_argument(
        '--total-questions',
        type=int,
        metavar='N',
        help='å®Ÿè¡Œã™ã‚‹ç·å•é¡Œæ•°ï¼ˆæŒ‡å®šã—ãŸæ•°ã ã‘é †ç•ªã«å®Ÿè¡Œï¼‰'
    )
    
    parser.add_argument(
        '--log-dir',
        default='logs',
        help='ãƒ­ã‚°å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: logs)'
    )
    
    args = parser.parse_args()
    
    # ãƒãƒŠãƒ¼è¡¨ç¤º
    print_banner()
    
    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†
    if args.test_mode:
        print("ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™")
        args.questions_per_category = 2
        args.max_turns = 8
        args.batch_size = 2
    elif args.experiment_mode:
        print(f"ğŸ”¬ å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ï¼ˆå„ã‚«ãƒ†ã‚´ãƒª{args.experiment_mode}å•ãšã¤ï¼‰")
        args.questions_per_category = args.experiment_mode
        args.max_turns = min(args.max_turns, 10)  # å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ã§ã¯çŸ­ã‚ã«
        args.batch_size = min(args.batch_size, args.experiment_mode)
    elif args.total_questions:
        print(f"ğŸ“Š æŒ‡å®šå•é¡Œæ•°ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ï¼ˆç·{args.total_questions}å•ï¼‰")
        # questions_per_categoryã¯å¾Œã§èª¿æ•´
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
    if not args.output_dir:
        args.output_dir = create_output_directory()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
    if not os.path.exists(args.dataset):
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.dataset}")
        sys.exit(1)
    
    # max_concurrentãŒbatch_sizeã‚ˆã‚Šå°ã•ã„å ´åˆã¯è‡ªå‹•èª¿æ•´
    effective_max_concurrent = max(args.max_concurrent, args.batch_size)
    if effective_max_concurrent != args.max_concurrent:
        print(f"âš ï¸  max_concurrent ã‚’ {args.max_concurrent} ã‹ã‚‰ {effective_max_concurrent} ã«è‡ªå‹•èª¿æ•´ã—ã¾ã—ãŸï¼ˆãƒãƒƒãƒã‚µã‚¤ã‚ºã«åˆã‚ã›ã¦ä¸¦è¡Œå®Ÿè¡Œï¼‰")
    
    # è¨­å®šä½œæˆ
    config = BenchmarkConfig(
        dataset_path=args.dataset,
        output_dir=args.output_dir,
        max_turns_per_question=args.max_turns,
        questions_per_category=args.questions_per_category,
        batch_size=args.batch_size,
        max_concurrent=effective_max_concurrent,
        timeout_per_question=args.timeout,
        save_intermediate_results=True,
        log_dir=args.log_dir
    )
    
    # è¨­å®šè¡¨ç¤º
    print_config_summary(config)
    
    # ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ãƒ»å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ä»¥å¤–ï¼‰
    if not args.test_mode and not args.experiment_mode:
        if args.total_questions:
            total_questions = args.total_questions
        else:
            total_questions = args.questions_per_category * 4  # 4ã‚«ãƒ†ã‚´ãƒªæƒ³å®š
            
        estimated_time = total_questions * args.max_turns * 2 / 60  # æ¦‚ç®—æ™‚é–“ï¼ˆåˆ†ï¼‰
        
        print(f"ğŸ“Š äºˆæƒ³å®Ÿè¡Œå†…å®¹:")
        print(f"  ç·å•é¡Œæ•°: {total_questions}å•")
        print(f"  æ¨å®šå®Ÿè¡Œæ™‚é–“: {estimated_time:.0f}åˆ†")
        print()
        
        response = input("å®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/N): ")
        if response.lower() not in ['y', 'yes']:
            print("å®Ÿè¡Œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚")
            sys.exit(0)
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    print("ğŸš€ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’é–‹å§‹ã—ã¾ã™...")
    print()
    
    runner = MMLUBenchmarkRunner(config)
    
    try:
        if args.resume:
            print("ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã—ã¾ã™...")
            report = await runner.resume_benchmark()
        else:
            # ç·å•é¡Œæ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’æ¸¡ã™
            total_questions = args.total_questions if args.total_questions else None
            report = await runner.run_full_benchmark(total_questions=total_questions)
        
        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print()
        print("="*80)
        print("ğŸ‰ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†!")
        print("="*80)
        print(f"ğŸ“ˆ çµæœã‚µãƒãƒªãƒ¼:")
        print(f"  ç·å•é¡Œæ•°: {report.total_questions}")
        print(f"  æ­£ç­”æ•°: {report.correct_answers}")
        print(f"  å…¨ä½“æ­£ç­”ç‡: {report.overall_accuracy:.2%}")
        print(f"  ä¿¡é ¼åº¦é‡ã¿ä»˜ãæ­£ç­”ç‡: {report.confidence_weighted_accuracy:.2%}")
        print(f"  å¹³å‡è­°è«–ã‚¿ãƒ¼ãƒ³æ•°: {report.average_turns:.1f}")
        print(f"  å¹³å‡å‡¦ç†æ™‚é–“: {report.average_processing_time:.1f}ç§’")
        print(f"  ç·å®Ÿè¡Œæ™‚é–“: {report.execution_time:.1f}ç§’")
        print()
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ
        print("ğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥æ­£ç­”ç‡:")
        for category, accuracy in report.category_accuracies.items():
            count = report.category_counts[category]
            correct = int(accuracy * count)
            print(f"  {category}: {correct}/{count} ({accuracy:.2%})")
        print()
        
        print(f"ğŸ“ è©³ç´°çµæœã¯ä»¥ä¸‹ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ:")
        print(f"  {config.output_dir}")
        print()
        
        return 0
        
    except KeyboardInterrupt:
        print()
        print("â¹ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        print("ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ä¸­...")
        
        try:
            await runner._save_checkpoint()
            print("âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            print("ğŸ”„ --resumeã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å†é–‹ã§ãã¾ã™")
        except Exception as save_error:
            print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {save_error}")
        
        return 130  # SIGINT
        
    except Exception as e:
        print()
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ä¸­...")
        
        try:
            await runner._save_checkpoint()
            print("âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ")
        except Exception as save_error:
            print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {save_error}")
        
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º
        import traceback
        print()
        print("ğŸ› ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
        traceback.print_exc()
        
        return 1


def run_progress_monitor():
    """é€²æ—ç›£è¦–ã®ç°¡æ˜“ç‰ˆï¼ˆå°†æ¥ã®æ‹¡å¼µç”¨ï¼‰"""
    # å®Ÿè£…äºˆå®šï¼šãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤º
    pass


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  å®Ÿè¡ŒãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)